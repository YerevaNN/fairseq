{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayane/miniconda3/envs/rk/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-07-14 09:38:40 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-07-14 09:38:40 | INFO | numexpr.utils | NumExpr defaulting to 4 threads.\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data.data_utils import load_indexed_dataset\n",
    "from fairseq.models.bart import BARTModel\n",
    "from fairseq.data import Dictionary\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "# os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "# os.system('CUDA_LAUNCH_BLOCKING=0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 09:38:47 | INFO | fairseq.file_utils | loading archive file /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/good/gayane/data/chkpt/BBBP_bs_16_dropout_0.1_lr_3e-5_totalNum_1020_warmup_163_noise_type_uniform_r3f_lambda_1.0/checkpoint_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 09:38:55 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 1026 types\n",
      "2022-07-14 09:38:55 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2022-07-14 09:39:00 | INFO | fairseq.models.bart.model | Registering classification head: sentence_classification_head\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BARTHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): BARTModel(\n",
       "      (encoder): TransformerEncoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(1026, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(130, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): TransformerDecoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(1026, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(130, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (output_projection): Linear(in_features=1024, out_features=1026, bias=False)\n",
       "      )\n",
       "      (classification_heads): ModuleDict(\n",
       "        (sentence_classification_head): BARTClassificationHead(\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): BARTModel(\n",
       "    (encoder): TransformerEncoderBase(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(1026, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(130, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoderBase(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(1026, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(130, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (output_projection): Linear(in_features=1024, out_features=1026, bias=False)\n",
       "    )\n",
       "    (classification_heads): ModuleDict(\n",
       "      (sentence_classification_head): BARTClassificationHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'BBBP'\n",
    "# dataset = dataset_name if dataset_name in set([\"BBBP\", \"BACE\", \"HIV\"]) else f\"{dataset_name}_{args.subtask}\"\n",
    "dataset = dataset_name\n",
    "\n",
    "store_path = \"/home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data\"\n",
    "model = f\"{store_path}/{dataset}/processed\"\n",
    "\n",
    "with open('/home/gayane/BartLM/captum/fairseq/scripts/datasets.json') as f:\n",
    "    datasets_json = json.load(f)\n",
    "dataset_js = datasets_json[dataset]\n",
    "task_type = dataset_js['type']\n",
    "\n",
    "if task_type == \"regression\":\n",
    "    mi = dataset_js['minimum']\n",
    "    ma = dataset_js['maximum']\n",
    "\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/\")\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/processed/\")\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/processed/input0/\")\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/processed/label/\")\n",
    "\n",
    "warmup = 163\n",
    "totNumUpdate = 1020\n",
    "lr = '3e-5'\n",
    "chkpt_path = '/mnt/good/gayane/data/chkpt/BBBP_bs_16_dropout_0.1_lr_3e-5_totalNum_1020_warmup_163_noise_type_uniform_r3f_lambda_1.0/checkpoint_best.pt'\n",
    "# chkpt_path = f\"/mnt/good/gayane/data/chkpt/{dataset}_bs_16_lr_{lr}_totalNum_{totNumUpdate}_warmup_{warmup}/checkpoint_last.pt\"\n",
    "print(chkpt_path)  # BACE_bs_16_lr_3e-5_totalNum_1135_warmup_181/ in test \n",
    "bart = BARTModel.from_pretrained(model,  checkpoint_file = chkpt_path, \n",
    "                                 bpe=\"sentencepiece\",\n",
    "                                 sentencepiece_model=\"/home/gayane/BartLM/Bart/chemical/tokenizer/chem.model\")\n",
    "bart.eval()\n",
    "bart.cuda(device=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bart.cuda(device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 09:57:55 | INFO | fairseq.data.data_utils | loaded 204 examples from: /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/input0/test\n",
      "2022-07-14 09:57:55 | INFO | fairseq.data.data_utils | loaded 204 examples from: /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/label/test\n",
      "2022-07-14 09:57:55 | INFO | fairseq.data.data_utils | loaded 204 examples from: /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/label/test\n"
     ]
    }
   ],
   "source": [
    "data_type = 'test'\n",
    "\n",
    "input_dict = Dictionary.load(f\"{store_path}/{dataset}/processed/input0/dict.txt\")\n",
    "smiles = list(load_indexed_dataset(\n",
    "    f\"{store_path}/{dataset}/processed/input0/{data_type}\", input_dict))\n",
    "\n",
    "\n",
    "test_label_path = f\"{store_path}/{dataset}/processed/label/{data_type}\"\n",
    "\n",
    "if task_type == 'classification':\n",
    "    \n",
    "    target_dict = Dictionary.load(f\"{store_path}/{dataset}/processed/label/dict.txt\")\n",
    "    targets = list(load_indexed_dataset(test_label_path, target_dict))\n",
    "elif task_type == 'regression':\n",
    "    with open(f'{test_label_path}.label') as f:\n",
    "        lines = f.readlines()\n",
    "        targets = [float(x.strip()) for x in lines]\n",
    "if task_type == 'classification':\n",
    "    if len(dataset_js[\"class_index\"])>1:\n",
    "        target_dict = list()\n",
    "        targets_list = list()\n",
    "        for i in range(len(dataset_js[\"class_index\"])):\n",
    "            target_dict.append(Dictionary.load(f\"{store_path}/{dataset}_{i}/processed/label/dict.txt\"))\n",
    "            targets_list.append(list(load_indexed_dataset(test_label_path[i], target_dict[i])))\n",
    "        \n",
    "    else: \n",
    "        target_dict = Dictionary.load(f\"{store_path}/{dataset}/processed/label/dict.txt\")\n",
    "        targets = list(load_indexed_dataset(test_label_path, target_dict))\n",
    "elif task_type == 'regression':\n",
    "    with open(f'{test_label_path}.label') as f:\n",
    "        lines = f.readlines()\n",
    "        targets = [float(x.strip()) for x in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 204/204 [00:00<00:00, 43621.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, (smile, target) in tqdm(list(enumerate(zip(smiles, targets)))):\n",
    "    smile = torch.cat((torch.cat((torch.tensor([0]), smile[:126])), torch.tensor([2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCC.[H+]'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.decode(bart.encode(\"CCC.[H+]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_718867/2256499831.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  smi.append(bart.decode(torch.tensor(sm)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N2CCC[C@H]2C(O)=O',\n",
       " 'FC(F)(F)[C@]1(OC(=O)Nc2ccc(Cl)cc12)C#CC3CC3',\n",
       " 'CN1C[C@@H](C[C@H]2[C@H]1Cc3c[nH]c4cccc2c34)C(=O)N[C@]5(C)O[C@@]6(O)[C@@H]7CCCN7C(=O)[C@H](Cc8ccccc8)N6C5=O']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smi = []\n",
    "for sm in smiles:\n",
    "    smi.append(bart.decode(torch.tensor(sm)))\n",
    "\n",
    "smi[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CC.OC(=O[C@H](CCc1ccccc1)N[C@@H(C)C(=O)N2CCC[C@H]2C(O)=O'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.decode(bart.encode('CCOC(=O[C@H](CCc1ccccc1)N[C@@H(C)C(=O)N2CCC[C@H]2C(O)=O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = pd.read_csv(f\"/home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/raw/{data_type}.input\", names=['SMILES'], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N2CCC[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FC(F)(F)[C@]1(OC(=O)Nc2ccc(Cl)cc12)C#CC3CC3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CN1C[C@@H](C[C@H]2[C@H]1Cc3c[nH]c4cccc2c34)C(=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN1CCC[C@H]1c2cccnc2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CN1CCC(=CC1)c2ccccc2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Nc1nnc(c(N)n1)c2cccc(Cl)c2Cl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>COc1ccc(Cl)cc1C(=O)NCCc2ccc(cc2)[S](=O)(=O)NC(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Cn1c2CCC(Cn3ccnc3C)C(=O)c2c4ccccc14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>CN(C)[C@H]1[C@@H]2C[C@H]3C(=C(O)c4c(O)cccc4[C@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SCC(=C(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                SMILES\n",
       "0    CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N2CCC[...\n",
       "1          FC(F)(F)[C@]1(OC(=O)Nc2ccc(Cl)cc12)C#CC3CC3\n",
       "2    CN1C[C@@H](C[C@H]2[C@H]1Cc3c[nH]c4cccc2c34)C(=...\n",
       "3                                 CN1CCC[C@H]1c2cccnc2\n",
       "4                                 CN1CCC(=CC1)c2ccccc2\n",
       "..                                                 ...\n",
       "199                       Nc1nnc(c(N)n1)c2cccc(Cl)c2Cl\n",
       "200  COc1ccc(Cl)cc1C(=O)NCCc2ccc(cc2)[S](=O)(=O)NC(...\n",
       "201                Cn1c2CCC(Cn3ccnc3C)C(=O)c2c4ccccc14\n",
       "202  CN(C)[C@H]1[C@@H]2C[C@H]3C(=C(O)c4c(O)cccc4[C@...\n",
       "203  CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SCC(=C(...\n",
       "\n",
       "[204 rows x 1 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9972198009490967"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = bart.encode('CCOC(=O)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N2CCC[C@H]2C(O)=O')\n",
    "output = bart.predict('sentence_classification_head', e )\n",
    "# target = target[0].item()\n",
    "output[0][1].exp().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.8841e+00, -2.7873e-03]], device='cuda:1',\n",
       "        grad_fn=<LogSoftmaxBackward0>),\n",
       " 0.9972166419029236)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = bart.encode('CCOC(=)[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N2CCC[C@H]2C(O)=O')\n",
    "output = bart.predict('sentence_classification_head', e )\n",
    "# target = target[0].item()\n",
    "output, output[0][1].exp().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9972227215766907"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = bart.encode('CCOC()[C@H](CCc1ccccc1)N[C@@H](C)C(=O)N2CCC[C@H]2C(O)=O')\n",
    "output = bart.predict('sentence_classification_head', e )\n",
    "# target = target[0].item()\n",
    "output[0][1].exp().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# li = valid_input[\"SMILES\"].to_list()\n",
    "li = _input[\"SMILES\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_smile = []\n",
    "false_recons = []\n",
    "for i in range(len(li)):\n",
    "    if smi[i] != li[i]:\n",
    "        false_smile.append(li[i])\n",
    "        false_recons.append(smi[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"Original SMILES\": false_smile, \"Reconstracted SMILES\": false_recons}\n",
    "df_bbbp = pd.DataFrame(d)\n",
    "# df_valid_bbbp = pd.DataFrame(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 17, 187)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(li), df_bbbp.shape[0], len(li) - df_bbbp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original SMILES</th>\n",
       "      <th>Reconstracted SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C[S](O)(=O)=O.Oc1ccc2[nH]cc(CCCCN3CCC(=CC3)c4c...</td>\n",
       "      <td>C[S](O)(=O)=O&lt;unk&gt;Oc1ccc2[nH]cc(CCCCN3CCC(=CC3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN1C2CCC1CC(C2)OC(c3ccccc3)c4ccccc4.O[S](O)(=O)=O</td>\n",
       "      <td>CN1C2CCC1CC(C2)OC(c3ccccc3)c4ccccc4&lt;unk&gt;O[S](O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OC(=O)\\C=C/C(O)=O.OC(=O)\\C=C/C(O)=O.C1CCN(CC1)...</td>\n",
       "      <td>OC(=O)\\C=C/C(O)=O&lt;unk&gt;OC(=O)\\C=C/C(O)=O&lt;unk&gt;C1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN[C@H](CC(C)C)C(=O)NC1[C@H](O)c2ccc(Oc3cc4cc(...</td>\n",
       "      <td>CN[C@H](CC(C)C)C(=O)NC1[C@H](O)c2ccc(Oc3cc4cc(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Ca++].NC1=NC(=O)C2=C(NCC(CNc3ccc(cc3)C(=O)N[C...</td>\n",
       "      <td>[C&lt;unk&gt;++]&lt;unk&gt;NC1=NC(=O)C2=C(NCC(CNc3ccc(cc3)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Br-].C[N+]1(C)CCCC(C1)OC(=O)C(O)(c2ccccc2)c3c...</td>\n",
       "      <td>[Br-]&lt;unk&gt;C[N+]1(C)CCCC(C1)OC(=O)C(O)(c2ccccc2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Br-].CN(C)C(=O)Oc1ccc[n+](C)c1</td>\n",
       "      <td>[Br-]&lt;unk&gt;CN(C)C(=O)Oc1ccc[n+](C)c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Br-].C[N+]1(C)CCC(C1)OC(=O)C(O)(C2CCCC2)c3ccccc3</td>\n",
       "      <td>[Br-]&lt;unk&gt;C[N+]1(C)CCC(C1)OC(=O)C(O)(C2CCCC2)c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Br-].C[N+]12CCC(CC1)C(C2)OC(=O)C(O)(c3ccccc3)...</td>\n",
       "      <td>[Br-]&lt;unk&gt;C[N+]12CCC(CC1)C(C2)OC(=O)C(O)(c3ccc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O.COC1CN(CCCOc2ccc(F)cc2)CCC1NC(=O)c3cc(Cl)c(N...</td>\n",
       "      <td>O&lt;unk&gt;COC1CN(CCCOc2ccc(F)cc2)CCC1NC(=O)c3cc(Cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Cl].CCCCOc1ccc(cc1)C(=O)CCN2CCCCC2</td>\n",
       "      <td>[Cl]&lt;unk&gt;CCCCOc1ccc(cc1)C(=O)CCN2CCCCC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CC[N+](C)(C)Cc1ccccc1Br.Cc2ccc(cc2)[S]([O-])(=...</td>\n",
       "      <td>CC[N+](C)(C)Cc1ccccc1Br&lt;unk&gt;Cc2ccc(cc2)[S]([O-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[H+].[Cl-].Clc1ccc2Sc3ccccc3N(CCCN4CCC5(CC4)NC...</td>\n",
       "      <td>[H+]&lt;unk&gt;[Cl-]&lt;unk&gt;Clc1ccc2Sc3ccccc3N(CCCN4CCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Na+].CCOc1ccc2ccccc2c1C(=O)N[C@H]3[C@H]4SC(C)...</td>\n",
       "      <td>[N&lt;unk&gt;+]&lt;unk&gt;CCOc1ccc2ccccc2c1C(=O)N[C@H]3[C@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>OC(=O)CCC(O)=O.FC(F)(F)c1ccc2Sc3ccccc3N(CCCN4C...</td>\n",
       "      <td>OC(=O)CCC(O)=O&lt;unk&gt;FC(F)(F)c1ccc2Sc3ccccc3N(CC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CN1C(=O)N(C)c2nc[nH]c2C1=O.CN3C(=O)N(C)c4nc[nH...</td>\n",
       "      <td>CN1C(=O)N(C)c2nc[nH]c2C1=O&lt;unk&gt;CN3C(=O)N(C)c4n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>O.CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SC(C)...</td>\n",
       "      <td>O&lt;unk&gt;CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Original SMILES  \\\n",
       "0   C[S](O)(=O)=O.Oc1ccc2[nH]cc(CCCCN3CCC(=CC3)c4c...   \n",
       "1   CN1C2CCC1CC(C2)OC(c3ccccc3)c4ccccc4.O[S](O)(=O)=O   \n",
       "2   OC(=O)\\C=C/C(O)=O.OC(=O)\\C=C/C(O)=O.C1CCN(CC1)...   \n",
       "3   CN[C@H](CC(C)C)C(=O)NC1[C@H](O)c2ccc(Oc3cc4cc(...   \n",
       "4   [Ca++].NC1=NC(=O)C2=C(NCC(CNc3ccc(cc3)C(=O)N[C...   \n",
       "5   [Br-].C[N+]1(C)CCCC(C1)OC(=O)C(O)(c2ccccc2)c3c...   \n",
       "6                     [Br-].CN(C)C(=O)Oc1ccc[n+](C)c1   \n",
       "7   [Br-].C[N+]1(C)CCC(C1)OC(=O)C(O)(C2CCCC2)c3ccccc3   \n",
       "8   [Br-].C[N+]12CCC(CC1)C(C2)OC(=O)C(O)(c3ccccc3)...   \n",
       "9   O.COC1CN(CCCOc2ccc(F)cc2)CCC1NC(=O)c3cc(Cl)c(N...   \n",
       "10                [Cl].CCCCOc1ccc(cc1)C(=O)CCN2CCCCC2   \n",
       "11  CC[N+](C)(C)Cc1ccccc1Br.Cc2ccc(cc2)[S]([O-])(=...   \n",
       "12  [H+].[Cl-].Clc1ccc2Sc3ccccc3N(CCCN4CCC5(CC4)NC...   \n",
       "13  [Na+].CCOc1ccc2ccccc2c1C(=O)N[C@H]3[C@H]4SC(C)...   \n",
       "14  OC(=O)CCC(O)=O.FC(F)(F)c1ccc2Sc3ccccc3N(CCCN4C...   \n",
       "15  CN1C(=O)N(C)c2nc[nH]c2C1=O.CN3C(=O)N(C)c4nc[nH...   \n",
       "16  O.CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SC(C)...   \n",
       "\n",
       "                                 Reconstracted SMILES  \n",
       "0   C[S](O)(=O)=O<unk>Oc1ccc2[nH]cc(CCCCN3CCC(=CC3...  \n",
       "1   CN1C2CCC1CC(C2)OC(c3ccccc3)c4ccccc4<unk>O[S](O...  \n",
       "2   OC(=O)\\C=C/C(O)=O<unk>OC(=O)\\C=C/C(O)=O<unk>C1...  \n",
       "3   CN[C@H](CC(C)C)C(=O)NC1[C@H](O)c2ccc(Oc3cc4cc(...  \n",
       "4   [C<unk>++]<unk>NC1=NC(=O)C2=C(NCC(CNc3ccc(cc3)...  \n",
       "5   [Br-]<unk>C[N+]1(C)CCCC(C1)OC(=O)C(O)(c2ccccc2...  \n",
       "6                 [Br-]<unk>CN(C)C(=O)Oc1ccc[n+](C)c1  \n",
       "7   [Br-]<unk>C[N+]1(C)CCC(C1)OC(=O)C(O)(C2CCCC2)c...  \n",
       "8   [Br-]<unk>C[N+]12CCC(CC1)C(C2)OC(=O)C(O)(c3ccc...  \n",
       "9   O<unk>COC1CN(CCCOc2ccc(F)cc2)CCC1NC(=O)c3cc(Cl...  \n",
       "10            [Cl]<unk>CCCCOc1ccc(cc1)C(=O)CCN2CCCCC2  \n",
       "11  CC[N+](C)(C)Cc1ccccc1Br<unk>Cc2ccc(cc2)[S]([O-...  \n",
       "12  [H+]<unk>[Cl-]<unk>Clc1ccc2Sc3ccccc3N(CCCN4CCC...  \n",
       "13  [N<unk>+]<unk>CCOc1ccc2ccccc2c1C(=O)N[C@H]3[C@...  \n",
       "14  OC(=O)CCC(O)=O<unk>FC(F)(F)c1ccc2Sc3ccccc3N(CC...  \n",
       "15  CN1C(=O)N(C)c2nc[nH]c2C1=O<unk>CN3C(=O)N(C)c4n...  \n",
       "16  O<unk>CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3S...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bbbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[Cl].CC[C@H]1CN2CCc3cc(OC)c(OC)cc3[C@@H]2C[C@@H]1C[C@H]4NCCc5cc(OC)c(OC)cc45',\n",
       " '[Cl].CC[C@H]1CN2CCc3cc(OC)c(OC)cc3[C@@H]2C[C@@H]1C[C@H]4NCCc5cc(OC)c(OC)cc45')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_bbbp.values[-1][0], df_bbbp.values[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bart' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[43mbart\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(bart\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCCOC(=)<mask>(CCc1ccccc1)N[C@@H](C)C(=O)N2CCC[C@H]2C(O)=O\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bart' is not defined"
     ]
    }
   ],
   "source": [
    "e = bart.decode(bart.encode('CCOC(=)<mask>(CCc1ccccc1)N[C@@H](C)C(=O)N2CCC[C@H]2C(O)=O'))\n",
    "# print(e)\n",
    "# bart.eval()\n",
    "# bart.fill_mask(masked_inputs = ['CCOC<mask>'],  topk=3, beam=10, match_source_len=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = bart.encode(\"CCOC(=O)(CCc1ccccc1)N[C@@H](C)C(=O)N2CCC[C@H]2C(O)=O\")\n",
    "last_layer_features = bart.extract_features(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 1024])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 16:46:21 | INFO | fairseq.file_utils | loading archive file /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/good/gayane/data/chkpt/BBBP_bs_16_dropout_0.1_lr_3e-5_totalNum_1020_warmup_163_noise_type_uniform_r3f_lambda_1.0/checkpoint_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-15 16:46:23 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 1026 types\n",
      "2022-07-15 16:46:23 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
      "2022-07-15 16:46:28 | INFO | fairseq.models.bart.model | Registering classification head: sentence_classification_head\n",
      "2022-07-15 16:46:29 | INFO | fairseq.data.data_utils | loaded 204 examples from: /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/input0/valid\n",
      "2022-07-15 16:46:29 | INFO | fairseq.data.data_utils | loaded 204 examples from: /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/label/valid\n",
      "2022-07-15 16:46:29 | INFO | fairseq.data.data_utils | loaded 204 examples from: /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/label/valid\n",
      "100%|| 204/204 [00:05<00:00, 39.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data.data_utils import load_indexed_dataset\n",
    "from fairseq.models.bart import BARTModel\n",
    "from fairseq.data import Dictionary\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "# os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "# os.system('CUDA_LAUNCH_BLOCKING=0')\n",
    "\n",
    "dataset_name = 'BBBP'\n",
    "# dataset = dataset_name if dataset_name in set([\"BBBP\", \"BACE\", \"HIV\"]) else f\"{dataset_name}_{args.subtask}\"\n",
    "dataset = dataset_name\n",
    "\n",
    "store_path = \"/home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data\"\n",
    "model = f\"{store_path}/{dataset}/processed\"\n",
    "\n",
    "with open('/home/gayane/BartLM/captum/fairseq/scripts/datasets.json') as f:\n",
    "    datasets_json = json.load(f)\n",
    "dataset_js = datasets_json[dataset]\n",
    "task_type = dataset_js['type']\n",
    "\n",
    "if task_type == \"regression\":\n",
    "    mi = dataset_js['minimum']\n",
    "    ma = dataset_js['maximum']\n",
    "\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/\")\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/processed/\")\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/processed/input0/\")\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/processed/label/\")\n",
    "\n",
    "warmup = 163\n",
    "totNumUpdate = 1020\n",
    "lr = '3e-5'\n",
    "chkpt_path = '/mnt/good/gayane/data/chkpt/BBBP_bs_16_dropout_0.1_lr_3e-5_totalNum_1020_warmup_163_noise_type_uniform_r3f_lambda_1.0/checkpoint_best.pt'\n",
    "# chkpt_path = f\"/mnt/good/gayane/data/chkpt/{dataset}_bs_16_lr_{lr}_totalNum_{totNumUpdate}_warmup_{warmup}/checkpoint_last.pt\"\n",
    "print(chkpt_path)  # BACE_bs_16_lr_3e-5_totalNum_1135_warmup_181/ in test \n",
    "bart = BARTModel.from_pretrained(model,  checkpoint_file = chkpt_path, \n",
    "                                 bpe=\"sentencepiece\",\n",
    "                                 sentencepiece_model=\"/home/gayane/BartLM/Bart/chemical/tokenizer/chem.model\")\n",
    "bart.eval()\n",
    "bart.cuda(device=0)\n",
    "\n",
    "\n",
    "data_type = 'valid'\n",
    "\n",
    "input_dict = Dictionary.load(f\"{store_path}/{dataset}/processed/input0/dict.txt\")\n",
    "smiles = list(load_indexed_dataset(\n",
    "    f\"{store_path}/{dataset}/processed/input0/{data_type}\", input_dict))\n",
    "\n",
    "\n",
    "test_label_path = f\"{store_path}/{dataset}/processed/label/{data_type}\"\n",
    "\n",
    "if task_type == 'classification':\n",
    "    \n",
    "    target_dict = Dictionary.load(f\"{store_path}/{dataset}/processed/label/dict.txt\")\n",
    "    targets = list(load_indexed_dataset(test_label_path, target_dict))\n",
    "elif task_type == 'regression':\n",
    "    with open(f'{test_label_path}.label') as f:\n",
    "        lines = f.readlines()\n",
    "        targets = [float(x.strip()) for x in lines]\n",
    "if task_type == 'classification':\n",
    "    if len(dataset_js[\"class_index\"])>1:\n",
    "        target_dict = list()\n",
    "        targets_list = list()\n",
    "        for i in range(len(dataset_js[\"class_index\"])):\n",
    "            target_dict.append(Dictionary.load(f\"{store_path}/{dataset}_{i}/processed/label/dict.txt\"))\n",
    "            targets_list.append(list(load_indexed_dataset(test_label_path[i], target_dict[i])))\n",
    "        \n",
    "    else: \n",
    "        target_dict = Dictionary.load(f\"{store_path}/{dataset}/processed/label/dict.txt\")\n",
    "        targets = list(load_indexed_dataset(test_label_path, target_dict))\n",
    "elif task_type == 'regression':\n",
    "    with open(f'{test_label_path}.label') as f:\n",
    "        lines = f.readlines()\n",
    "        targets = [float(x.strip()) for x in lines]\n",
    "\n",
    "y_pred = []\n",
    "y = []\n",
    "sm = []\n",
    "for i, (smile, target) in tqdm(list(enumerate(zip(smiles, targets)))):\n",
    "    smile = torch.cat((torch.cat((torch.tensor([0]), smile[:126])), torch.tensor([2])))  \n",
    "    if task_type ==\"classification\":\n",
    "        output = bart.predict('sentence_classification_head', smile)\n",
    "        target = target[0].item()\n",
    "        y_pred.append(output[0][1].exp().item())\n",
    "        y.append(target - 4)\n",
    "        sm.append(bart.decode(smile))\n",
    "        \n",
    "    elif task_type == \"regression\":\n",
    "        output = bart.predict('sentence_classification_head', smile, return_logits=True)\n",
    "        y_pred.append(output[0][0].item())\n",
    "        y.append(target)\n",
    "        sm.append(bart.decode(smile))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_input = pd.read_csv(f\"/home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/raw/{data_type}.input\", names=['SMILES'], header=None)\n",
    "print(len(_input))\n",
    "\n",
    "li = _input[\"SMILES\"].to_list()\n",
    "l = _input[\"SMILES\"].to_list()\n",
    "\n",
    "smi = []\n",
    "for sml in range(len(li)):\n",
    "    if \"(\" in li[sml]:\n",
    "        li[sml] = li[sml].replace('(', \"\", 1)\n",
    "for s in li:\n",
    "    smi.append(bart.decode(bart.encode(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[H+].C2=C1C(OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " '[H+].C2=C1COC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " '[H+].C2=C1COC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0], li[0], smi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "li = _input[\"SMILES\"].to_list()\n",
    "l = _input[\"SMILES\"].to_list()\n",
    "\n",
    "smi = []\n",
    "for sml in range(len(li)):\n",
    "    if \"(\" in li[sml]:\n",
    "        li[sml] = li[sml].replace('(', \"\", 1)\n",
    "for s in li:\n",
    "    smi.append(bart.decode(bart.encode(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "unterminated character set at position 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[0;32m----> 5\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43me<=1}\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     a \u001b[38;5;241m=\u001b[39m m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnomatch\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(a)\n",
      "File \u001b[0;32m~/miniconda3/envs/rk/lib/python3.9/site-packages/regex/regex.py:267\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags, pos, endpos, partial, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endpos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    264\u001b[0m   concurrent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;124;03m\"\"\"Search through string looking for a match to the pattern, returning a\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     pat \u001b[38;5;241m=\u001b[39m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pat\u001b[38;5;241m.\u001b[39msearch(string, pos, endpos, concurrent, partial, timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/rk/lib/python3.9/site-packages/regex/regex.py:532\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags, ignore_unused, kwargs, cache_it)\u001b[0m\n\u001b[1;32m    529\u001b[0m         caught_exception \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m caught_exception:\n\u001b[0;32m--> 532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error(caught_exception\u001b[38;5;241m.\u001b[39mmsg, caught_exception\u001b[38;5;241m.\u001b[39mpattern,\n\u001b[1;32m    533\u001b[0m           caught_exception\u001b[38;5;241m.\u001b[39mpos)\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source\u001b[38;5;241m.\u001b[39mat_end():\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munbalanced parenthesis\u001b[39m\u001b[38;5;124m\"\u001b[39m, pattern, source\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31merror\u001b[0m: unterminated character set at position 9"
     ]
    }
   ],
   "source": [
    "queries = '[H+].C2=C1C(OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]'\n",
    "seq =     '[H+].C2=C1COC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]'\n",
    "import regex as re\n",
    "for q in queries:\n",
    "    m = re.search(r'(%s){e<=1}'%q, seq)\n",
    "    a = m if m else 'nomatch'\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substrings(s1, s2):\n",
    "    final = [s1[i:b+1] for i in range(len(s1)) for b in range(len(s1))]\n",
    "\n",
    "\n",
    "    return [i for i in final if i in s1 and i in s2 and len(i) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[H',\n",
       " '[H+',\n",
       " '[H+]',\n",
       " '[H+].',\n",
       " '[H+].C',\n",
       " '[H+].C2',\n",
       " '[H+].C2=',\n",
       " '[H+].C2=C',\n",
       " '[H+].C2=C1',\n",
       " '[H+].C2=C1C',\n",
       " 'H+',\n",
       " 'H+]',\n",
       " 'H+].',\n",
       " 'H+].C',\n",
       " 'H+].C2',\n",
       " 'H+].C2=',\n",
       " 'H+].C2=C',\n",
       " 'H+].C2=C1',\n",
       " 'H+].C2=C1C',\n",
       " '+]',\n",
       " '+].',\n",
       " '+].C',\n",
       " '+].C2',\n",
       " '+].C2=',\n",
       " '+].C2=C',\n",
       " '+].C2=C1',\n",
       " '+].C2=C1C',\n",
       " '].',\n",
       " '].C',\n",
       " '].C2',\n",
       " '].C2=',\n",
       " '].C2=C',\n",
       " '].C2=C1',\n",
       " '].C2=C1C',\n",
       " '.C',\n",
       " '.C2',\n",
       " '.C2=',\n",
       " '.C2=C',\n",
       " '.C2=C1',\n",
       " '.C2=C1C',\n",
       " 'C2',\n",
       " 'C2=',\n",
       " 'C2=C',\n",
       " 'C2=C1',\n",
       " 'C2=C1C',\n",
       " '2=',\n",
       " '2=C',\n",
       " '2=C1',\n",
       " '2=C1C',\n",
       " '=C',\n",
       " '=C1',\n",
       " '=C1C',\n",
       " 'C1',\n",
       " 'C1C',\n",
       " '1C',\n",
       " 'C(',\n",
       " 'OC',\n",
       " 'OC(',\n",
       " 'OC(=',\n",
       " 'OC(=N',\n",
       " 'OC(=NC',\n",
       " 'OC(=NC1',\n",
       " 'OC(=NC1=',\n",
       " 'OC(=NC1=C',\n",
       " 'OC(=NC1=CC',\n",
       " 'OC(=NC1=CC=',\n",
       " 'OC(=NC1=CC=C',\n",
       " 'OC(=NC1=CC=C2',\n",
       " 'OC(=NC1=CC=C2C',\n",
       " 'OC(=NC1=CC=C2Cl',\n",
       " 'OC(=NC1=CC=C2Cl)',\n",
       " 'OC(=NC1=CC=C2Cl)N',\n",
       " 'OC(=NC1=CC=C2Cl)NC',\n",
       " 'OC(=NC1=CC=C2Cl)NCC',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=C',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=C',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'C(',\n",
       " 'C(=',\n",
       " 'C(=N',\n",
       " 'C(=NC',\n",
       " 'C(=NC1',\n",
       " 'C(=NC1=',\n",
       " 'C(=NC1=C',\n",
       " 'C(=NC1=CC',\n",
       " 'C(=NC1=CC=',\n",
       " 'C(=NC1=CC=C',\n",
       " 'C(=NC1=CC=C2',\n",
       " 'C(=NC1=CC=C2C',\n",
       " 'C(=NC1=CC=C2Cl',\n",
       " 'C(=NC1=CC=C2Cl)',\n",
       " 'C(=NC1=CC=C2Cl)N',\n",
       " 'C(=NC1=CC=C2Cl)NC',\n",
       " 'C(=NC1=CC=C2Cl)NCC',\n",
       " 'C(=NC1=CC=C2Cl)NCC)',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=C',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=C',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'C(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " '(=',\n",
       " '(=N',\n",
       " '(=NC',\n",
       " '(=NC1',\n",
       " '(=NC1=',\n",
       " '(=NC1=C',\n",
       " '(=NC1=CC',\n",
       " '(=NC1=CC=',\n",
       " '(=NC1=CC=C',\n",
       " '(=NC1=CC=C2',\n",
       " '(=NC1=CC=C2C',\n",
       " '(=NC1=CC=C2Cl',\n",
       " '(=NC1=CC=C2Cl)',\n",
       " '(=NC1=CC=C2Cl)N',\n",
       " '(=NC1=CC=C2Cl)NC',\n",
       " '(=NC1=CC=C2Cl)NCC',\n",
       " '(=NC1=CC=C2Cl)NCC)',\n",
       " '(=NC1=CC=C2Cl)NCC)(',\n",
       " '(=NC1=CC=C2Cl)NCC)(C',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=C',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=C',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " '(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " '=N',\n",
       " '=NC',\n",
       " '=NC1',\n",
       " '=NC1=',\n",
       " '=NC1=C',\n",
       " '=NC1=CC',\n",
       " '=NC1=CC=',\n",
       " '=NC1=CC=C',\n",
       " '=NC1=CC=C2',\n",
       " '=NC1=CC=C2C',\n",
       " '=NC1=CC=C2Cl',\n",
       " '=NC1=CC=C2Cl)',\n",
       " '=NC1=CC=C2Cl)N',\n",
       " '=NC1=CC=C2Cl)NC',\n",
       " '=NC1=CC=C2Cl)NCC',\n",
       " '=NC1=CC=C2Cl)NCC)',\n",
       " '=NC1=CC=C2Cl)NCC)(',\n",
       " '=NC1=CC=C2Cl)NCC)(C',\n",
       " '=NC1=CC=C2Cl)NCC)(C3',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=C',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=C',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " '=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'NC',\n",
       " 'NC1',\n",
       " 'NC1=',\n",
       " 'NC1=C',\n",
       " 'NC1=CC',\n",
       " 'NC1=CC=',\n",
       " 'NC1=CC=C',\n",
       " 'NC1=CC=C2',\n",
       " 'NC1=CC=C2C',\n",
       " 'NC1=CC=C2Cl',\n",
       " 'NC1=CC=C2Cl)',\n",
       " 'NC1=CC=C2Cl)N',\n",
       " 'NC1=CC=C2Cl)NC',\n",
       " 'NC1=CC=C2Cl)NCC',\n",
       " 'NC1=CC=C2Cl)NCC)',\n",
       " 'NC1=CC=C2Cl)NCC)(',\n",
       " 'NC1=CC=C2Cl)NCC)(C',\n",
       " 'NC1=CC=C2Cl)NCC)(C3',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=C',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=C',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'C1',\n",
       " 'C1=',\n",
       " 'C1=C',\n",
       " 'C1=CC',\n",
       " 'C1=CC=',\n",
       " 'C1=CC=C',\n",
       " 'C1=CC=C2',\n",
       " 'C1=CC=C2C',\n",
       " 'C1=CC=C2Cl',\n",
       " 'C1=CC=C2Cl)',\n",
       " 'C1=CC=C2Cl)N',\n",
       " 'C1=CC=C2Cl)NC',\n",
       " 'C1=CC=C2Cl)NCC',\n",
       " 'C1=CC=C2Cl)NCC)',\n",
       " 'C1=CC=C2Cl)NCC)(',\n",
       " 'C1=CC=C2Cl)NCC)(C',\n",
       " 'C1=CC=C2Cl)NCC)(C3',\n",
       " 'C1=CC=C2Cl)NCC)(C3=',\n",
       " 'C1=CC=C2Cl)NCC)(C3=C',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=C',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'C1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " '1=',\n",
       " '1=C',\n",
       " '1=CC',\n",
       " '1=CC=',\n",
       " '1=CC=C',\n",
       " '1=CC=C2',\n",
       " '1=CC=C2C',\n",
       " '1=CC=C2Cl',\n",
       " '1=CC=C2Cl)',\n",
       " '1=CC=C2Cl)N',\n",
       " '1=CC=C2Cl)NC',\n",
       " '1=CC=C2Cl)NCC',\n",
       " '1=CC=C2Cl)NCC)',\n",
       " '1=CC=C2Cl)NCC)(',\n",
       " '1=CC=C2Cl)NCC)(C',\n",
       " '1=CC=C2Cl)NCC)(C3',\n",
       " '1=CC=C2Cl)NCC)(C3=',\n",
       " '1=CC=C2Cl)NCC)(C3=C',\n",
       " '1=CC=C2Cl)NCC)(C3=CC',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=C',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " '1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " '=C',\n",
       " '=CC',\n",
       " '=CC=',\n",
       " '=CC=C',\n",
       " '=CC=C2',\n",
       " '=CC=C2C',\n",
       " '=CC=C2Cl',\n",
       " '=CC=C2Cl)',\n",
       " '=CC=C2Cl)N',\n",
       " '=CC=C2Cl)NC',\n",
       " '=CC=C2Cl)NCC',\n",
       " '=CC=C2Cl)NCC)',\n",
       " '=CC=C2Cl)NCC)(',\n",
       " '=CC=C2Cl)NCC)(C',\n",
       " '=CC=C2Cl)NCC)(C3',\n",
       " '=CC=C2Cl)NCC)(C3=',\n",
       " '=CC=C2Cl)NCC)(C3=C',\n",
       " '=CC=C2Cl)NCC)(C3=CC',\n",
       " '=CC=C2Cl)NCC)(C3=CC=',\n",
       " '=CC=C2Cl)NCC)(C3=CC=C',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " '=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'CC',\n",
       " 'CC=',\n",
       " 'CC=C',\n",
       " 'CC=C2',\n",
       " 'CC=C2C',\n",
       " 'CC=C2Cl',\n",
       " 'CC=C2Cl)',\n",
       " 'CC=C2Cl)N',\n",
       " 'CC=C2Cl)NC',\n",
       " 'CC=C2Cl)NCC',\n",
       " 'CC=C2Cl)NCC)',\n",
       " 'CC=C2Cl)NCC)(',\n",
       " 'CC=C2Cl)NCC)(C',\n",
       " 'CC=C2Cl)NCC)(C3',\n",
       " 'CC=C2Cl)NCC)(C3=',\n",
       " 'CC=C2Cl)NCC)(C3=C',\n",
       " 'CC=C2Cl)NCC)(C3=CC',\n",
       " 'CC=C2Cl)NCC)(C3=CC=',\n",
       " 'CC=C2Cl)NCC)(C3=CC=C',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'C=',\n",
       " 'C=C',\n",
       " 'C=C2',\n",
       " 'C=C2C',\n",
       " 'C=C2Cl',\n",
       " 'C=C2Cl)',\n",
       " 'C=C2Cl)N',\n",
       " 'C=C2Cl)NC',\n",
       " 'C=C2Cl)NCC',\n",
       " 'C=C2Cl)NCC)',\n",
       " 'C=C2Cl)NCC)(',\n",
       " 'C=C2Cl)NCC)(C',\n",
       " 'C=C2Cl)NCC)(C3',\n",
       " 'C=C2Cl)NCC)(C3=',\n",
       " 'C=C2Cl)NCC)(C3=C',\n",
       " 'C=C2Cl)NCC)(C3=CC',\n",
       " 'C=C2Cl)NCC)(C3=CC=',\n",
       " 'C=C2Cl)NCC)(C3=CC=C',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'C=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " '=C',\n",
       " '=C2',\n",
       " '=C2C',\n",
       " '=C2Cl',\n",
       " '=C2Cl)',\n",
       " '=C2Cl)N',\n",
       " '=C2Cl)NC',\n",
       " '=C2Cl)NCC',\n",
       " '=C2Cl)NCC)',\n",
       " '=C2Cl)NCC)(',\n",
       " '=C2Cl)NCC)(C',\n",
       " '=C2Cl)NCC)(C3',\n",
       " '=C2Cl)NCC)(C3=',\n",
       " '=C2Cl)NCC)(C3=C',\n",
       " '=C2Cl)NCC)(C3=CC',\n",
       " '=C2Cl)NCC)(C3=CC=',\n",
       " '=C2Cl)NCC)(C3=CC=C',\n",
       " '=C2Cl)NCC)(C3=CC=CC',\n",
       " '=C2Cl)NCC)(C3=CC=CC=',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C3',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " '=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'C2',\n",
       " 'C2C',\n",
       " 'C2Cl',\n",
       " 'C2Cl)',\n",
       " 'C2Cl)N',\n",
       " 'C2Cl)NC',\n",
       " 'C2Cl)NCC',\n",
       " 'C2Cl)NCC)',\n",
       " 'C2Cl)NCC)(',\n",
       " 'C2Cl)NCC)(C',\n",
       " 'C2Cl)NCC)(C3',\n",
       " 'C2Cl)NCC)(C3=',\n",
       " 'C2Cl)NCC)(C3=C',\n",
       " 'C2Cl)NCC)(C3=CC',\n",
       " 'C2Cl)NCC)(C3=CC=',\n",
       " 'C2Cl)NCC)(C3=CC=C',\n",
       " 'C2Cl)NCC)(C3=CC=CC',\n",
       " 'C2Cl)NCC)(C3=CC=CC=',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C3',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C3)',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " '2C',\n",
       " '2Cl',\n",
       " '2Cl)',\n",
       " '2Cl)N',\n",
       " '2Cl)NC',\n",
       " '2Cl)NCC',\n",
       " '2Cl)NCC)',\n",
       " '2Cl)NCC)(',\n",
       " '2Cl)NCC)(C',\n",
       " '2Cl)NCC)(C3',\n",
       " '2Cl)NCC)(C3=',\n",
       " '2Cl)NCC)(C3=C',\n",
       " '2Cl)NCC)(C3=CC',\n",
       " '2Cl)NCC)(C3=CC=',\n",
       " '2Cl)NCC)(C3=CC=C',\n",
       " '2Cl)NCC)(C3=CC=CC',\n",
       " '2Cl)NCC)(C3=CC=CC=',\n",
       " '2Cl)NCC)(C3=CC=CC=C',\n",
       " '2Cl)NCC)(C3=CC=CC=C3',\n",
       " '2Cl)NCC)(C3=CC=CC=C3)',\n",
       " '2Cl)NCC)(C3=CC=CC=C3)C',\n",
       " '2Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " '2Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " '2Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " '2Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " '2Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " '2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'Cl',\n",
       " 'Cl)',\n",
       " 'Cl)N',\n",
       " 'Cl)NC',\n",
       " 'Cl)NCC',\n",
       " 'Cl)NCC)',\n",
       " 'Cl)NCC)(',\n",
       " 'Cl)NCC)(C',\n",
       " 'Cl)NCC)(C3',\n",
       " 'Cl)NCC)(C3=',\n",
       " 'Cl)NCC)(C3=C',\n",
       " 'Cl)NCC)(C3=CC',\n",
       " 'Cl)NCC)(C3=CC=',\n",
       " 'Cl)NCC)(C3=CC=C',\n",
       " 'Cl)NCC)(C3=CC=CC',\n",
       " 'Cl)NCC)(C3=CC=CC=',\n",
       " 'Cl)NCC)(C3=CC=CC=C',\n",
       " 'Cl)NCC)(C3=CC=CC=C3',\n",
       " 'Cl)NCC)(C3=CC=CC=C3)',\n",
       " 'Cl)NCC)(C3=CC=CC=C3)C',\n",
       " 'Cl)NCC)(C3=CC=CC=C3)C.',\n",
       " 'Cl)NCC)(C3=CC=CC=C3)C.[',\n",
       " 'Cl)NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'Cl)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'Cl)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'Cl)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'l)',\n",
       " 'l)N',\n",
       " 'l)NC',\n",
       " 'l)NCC',\n",
       " 'l)NCC)',\n",
       " 'l)NCC)(',\n",
       " 'l)NCC)(C',\n",
       " 'l)NCC)(C3',\n",
       " 'l)NCC)(C3=',\n",
       " 'l)NCC)(C3=C',\n",
       " 'l)NCC)(C3=CC',\n",
       " 'l)NCC)(C3=CC=',\n",
       " 'l)NCC)(C3=CC=C',\n",
       " 'l)NCC)(C3=CC=CC',\n",
       " 'l)NCC)(C3=CC=CC=',\n",
       " 'l)NCC)(C3=CC=CC=C',\n",
       " 'l)NCC)(C3=CC=CC=C3',\n",
       " 'l)NCC)(C3=CC=CC=C3)',\n",
       " 'l)NCC)(C3=CC=CC=C3)C',\n",
       " 'l)NCC)(C3=CC=CC=C3)C.',\n",
       " 'l)NCC)(C3=CC=CC=C3)C.[',\n",
       " 'l)NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'l)NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'l)NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'l)NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " ')N',\n",
       " ')NC',\n",
       " ')NCC',\n",
       " ')NCC)',\n",
       " ')NCC)(',\n",
       " ')NCC)(C',\n",
       " ')NCC)(C3',\n",
       " ')NCC)(C3=',\n",
       " ')NCC)(C3=C',\n",
       " ')NCC)(C3=CC',\n",
       " ')NCC)(C3=CC=',\n",
       " ')NCC)(C3=CC=C',\n",
       " ')NCC)(C3=CC=CC',\n",
       " ')NCC)(C3=CC=CC=',\n",
       " ')NCC)(C3=CC=CC=C',\n",
       " ')NCC)(C3=CC=CC=C3',\n",
       " ')NCC)(C3=CC=CC=C3)',\n",
       " ')NCC)(C3=CC=CC=C3)C',\n",
       " ')NCC)(C3=CC=CC=C3)C.',\n",
       " ')NCC)(C3=CC=CC=C3)C.[',\n",
       " ')NCC)(C3=CC=CC=C3)C.[C',\n",
       " ')NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " ')NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " ')NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'NC',\n",
       " 'NCC',\n",
       " 'NCC)',\n",
       " 'NCC)(',\n",
       " 'NCC)(C',\n",
       " 'NCC)(C3',\n",
       " 'NCC)(C3=',\n",
       " 'NCC)(C3=C',\n",
       " 'NCC)(C3=CC',\n",
       " 'NCC)(C3=CC=',\n",
       " 'NCC)(C3=CC=C',\n",
       " 'NCC)(C3=CC=CC',\n",
       " 'NCC)(C3=CC=CC=',\n",
       " 'NCC)(C3=CC=CC=C',\n",
       " 'NCC)(C3=CC=CC=C3',\n",
       " 'NCC)(C3=CC=CC=C3)',\n",
       " 'NCC)(C3=CC=CC=C3)C',\n",
       " 'NCC)(C3=CC=CC=C3)C.',\n",
       " 'NCC)(C3=CC=CC=C3)C.[',\n",
       " 'NCC)(C3=CC=CC=C3)C.[C',\n",
       " 'NCC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'NCC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'NCC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'CC',\n",
       " 'CC)',\n",
       " 'CC)(',\n",
       " 'CC)(C',\n",
       " 'CC)(C3',\n",
       " 'CC)(C3=',\n",
       " 'CC)(C3=C',\n",
       " 'CC)(C3=CC',\n",
       " 'CC)(C3=CC=',\n",
       " 'CC)(C3=CC=C',\n",
       " 'CC)(C3=CC=CC',\n",
       " 'CC)(C3=CC=CC=',\n",
       " 'CC)(C3=CC=CC=C',\n",
       " 'CC)(C3=CC=CC=C3',\n",
       " 'CC)(C3=CC=CC=C3)',\n",
       " 'CC)(C3=CC=CC=C3)C',\n",
       " 'CC)(C3=CC=CC=C3)C.',\n",
       " 'CC)(C3=CC=CC=C3)C.[',\n",
       " 'CC)(C3=CC=CC=C3)C.[C',\n",
       " 'CC)(C3=CC=CC=C3)C.[Cl',\n",
       " 'CC)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'CC)(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'C)',\n",
       " 'C)(',\n",
       " 'C)(C',\n",
       " 'C)(C3',\n",
       " 'C)(C3=',\n",
       " 'C)(C3=C',\n",
       " 'C)(C3=CC',\n",
       " 'C)(C3=CC=',\n",
       " 'C)(C3=CC=C',\n",
       " 'C)(C3=CC=CC',\n",
       " 'C)(C3=CC=CC=',\n",
       " 'C)(C3=CC=CC=C',\n",
       " 'C)(C3=CC=CC=C3',\n",
       " 'C)(C3=CC=CC=C3)',\n",
       " 'C)(C3=CC=CC=C3)C',\n",
       " 'C)(C3=CC=CC=C3)C.',\n",
       " 'C)(C3=CC=CC=C3)C.[',\n",
       " 'C)(C3=CC=CC=C3)C.[C',\n",
       " 'C)(C3=CC=CC=C3)C.[Cl',\n",
       " 'C)(C3=CC=CC=C3)C.[Cl-',\n",
       " 'C)(C3=CC=CC=C3)C.[Cl-]',\n",
       " ')(',\n",
       " ')(C',\n",
       " ')(C3',\n",
       " ')(C3=',\n",
       " ')(C3=C',\n",
       " ')(C3=CC',\n",
       " ')(C3=CC=',\n",
       " ')(C3=CC=C',\n",
       " ')(C3=CC=CC',\n",
       " ')(C3=CC=CC=',\n",
       " ')(C3=CC=CC=C',\n",
       " ')(C3=CC=CC=C3',\n",
       " ')(C3=CC=CC=C3)',\n",
       " ')(C3=CC=CC=C3)C',\n",
       " ')(C3=CC=CC=C3)C.',\n",
       " ')(C3=CC=CC=C3)C.[',\n",
       " ')(C3=CC=CC=C3)C.[C',\n",
       " ')(C3=CC=CC=C3)C.[Cl',\n",
       " ')(C3=CC=CC=C3)C.[Cl-',\n",
       " ')(C3=CC=CC=C3)C.[Cl-]',\n",
       " '(C',\n",
       " '(C3',\n",
       " '(C3=',\n",
       " '(C3=C',\n",
       " '(C3=CC',\n",
       " '(C3=CC=',\n",
       " '(C3=CC=C',\n",
       " '(C3=CC=CC',\n",
       " '(C3=CC=CC=',\n",
       " '(C3=CC=CC=C',\n",
       " '(C3=CC=CC=C3',\n",
       " '(C3=CC=CC=C3)',\n",
       " '(C3=CC=CC=C3)C',\n",
       " '(C3=CC=CC=C3)C.',\n",
       " '(C3=CC=CC=C3)C.[',\n",
       " '(C3=CC=CC=C3)C.[C',\n",
       " '(C3=CC=CC=C3)C.[Cl',\n",
       " '(C3=CC=CC=C3)C.[Cl-',\n",
       " '(C3=CC=CC=C3)C.[Cl-]',\n",
       " 'C3',\n",
       " 'C3=',\n",
       " 'C3=C',\n",
       " 'C3=CC',\n",
       " 'C3=CC=',\n",
       " 'C3=CC=C',\n",
       " 'C3=CC=CC',\n",
       " 'C3=CC=CC=',\n",
       " 'C3=CC=CC=C',\n",
       " 'C3=CC=CC=C3',\n",
       " 'C3=CC=CC=C3)',\n",
       " 'C3=CC=CC=C3)C',\n",
       " 'C3=CC=CC=C3)C.',\n",
       " 'C3=CC=CC=C3)C.[',\n",
       " 'C3=CC=CC=C3)C.[C',\n",
       " 'C3=CC=CC=C3)C.[Cl',\n",
       " 'C3=CC=CC=C3)C.[Cl-',\n",
       " 'C3=CC=CC=C3)C.[Cl-]',\n",
       " '3=',\n",
       " '3=C',\n",
       " '3=CC',\n",
       " '3=CC=',\n",
       " '3=CC=C',\n",
       " '3=CC=CC',\n",
       " '3=CC=CC=',\n",
       " '3=CC=CC=C',\n",
       " '3=CC=CC=C3',\n",
       " '3=CC=CC=C3)',\n",
       " '3=CC=CC=C3)C',\n",
       " '3=CC=CC=C3)C.',\n",
       " '3=CC=CC=C3)C.[',\n",
       " '3=CC=CC=C3)C.[C',\n",
       " '3=CC=CC=C3)C.[Cl',\n",
       " '3=CC=CC=C3)C.[Cl-',\n",
       " '3=CC=CC=C3)C.[Cl-]',\n",
       " '=C',\n",
       " '=CC',\n",
       " '=CC=',\n",
       " '=CC=C',\n",
       " '=CC=CC',\n",
       " '=CC=CC=',\n",
       " '=CC=CC=C',\n",
       " '=CC=CC=C3',\n",
       " '=CC=CC=C3)',\n",
       " '=CC=CC=C3)C',\n",
       " '=CC=CC=C3)C.',\n",
       " '=CC=CC=C3)C.[',\n",
       " '=CC=CC=C3)C.[C',\n",
       " '=CC=CC=C3)C.[Cl',\n",
       " '=CC=CC=C3)C.[Cl-',\n",
       " '=CC=CC=C3)C.[Cl-]',\n",
       " 'CC',\n",
       " 'CC=',\n",
       " 'CC=C',\n",
       " 'CC=CC',\n",
       " 'CC=CC=',\n",
       " 'CC=CC=C',\n",
       " 'CC=CC=C3',\n",
       " 'CC=CC=C3)',\n",
       " 'CC=CC=C3)C',\n",
       " 'CC=CC=C3)C.',\n",
       " 'CC=CC=C3)C.[',\n",
       " 'CC=CC=C3)C.[C',\n",
       " 'CC=CC=C3)C.[Cl',\n",
       " 'CC=CC=C3)C.[Cl-',\n",
       " 'CC=CC=C3)C.[Cl-]',\n",
       " 'C=',\n",
       " 'C=C',\n",
       " 'C=CC',\n",
       " 'C=CC=',\n",
       " 'C=CC=C',\n",
       " 'C=CC=C3',\n",
       " 'C=CC=C3)',\n",
       " 'C=CC=C3)C',\n",
       " 'C=CC=C3)C.',\n",
       " 'C=CC=C3)C.[',\n",
       " 'C=CC=C3)C.[C',\n",
       " 'C=CC=C3)C.[Cl',\n",
       " 'C=CC=C3)C.[Cl-',\n",
       " 'C=CC=C3)C.[Cl-]',\n",
       " '=C',\n",
       " '=CC',\n",
       " '=CC=',\n",
       " '=CC=C',\n",
       " '=CC=C3',\n",
       " '=CC=C3)',\n",
       " '=CC=C3)C',\n",
       " '=CC=C3)C.',\n",
       " '=CC=C3)C.[',\n",
       " '=CC=C3)C.[C',\n",
       " '=CC=C3)C.[Cl',\n",
       " '=CC=C3)C.[Cl-',\n",
       " '=CC=C3)C.[Cl-]',\n",
       " 'CC',\n",
       " 'CC=',\n",
       " 'CC=C',\n",
       " 'CC=C3',\n",
       " 'CC=C3)',\n",
       " 'CC=C3)C',\n",
       " 'CC=C3)C.',\n",
       " 'CC=C3)C.[',\n",
       " 'CC=C3)C.[C',\n",
       " 'CC=C3)C.[Cl',\n",
       " 'CC=C3)C.[Cl-',\n",
       " 'CC=C3)C.[Cl-]',\n",
       " 'C=',\n",
       " 'C=C',\n",
       " 'C=C3',\n",
       " 'C=C3)',\n",
       " 'C=C3)C',\n",
       " 'C=C3)C.',\n",
       " 'C=C3)C.[',\n",
       " 'C=C3)C.[C',\n",
       " 'C=C3)C.[Cl',\n",
       " 'C=C3)C.[Cl-',\n",
       " 'C=C3)C.[Cl-]',\n",
       " '=C',\n",
       " '=C3',\n",
       " '=C3)',\n",
       " '=C3)C',\n",
       " '=C3)C.',\n",
       " '=C3)C.[',\n",
       " '=C3)C.[C',\n",
       " '=C3)C.[Cl',\n",
       " '=C3)C.[Cl-',\n",
       " '=C3)C.[Cl-]',\n",
       " 'C3',\n",
       " 'C3)',\n",
       " 'C3)C',\n",
       " 'C3)C.',\n",
       " 'C3)C.[',\n",
       " 'C3)C.[C',\n",
       " 'C3)C.[Cl',\n",
       " 'C3)C.[Cl-',\n",
       " 'C3)C.[Cl-]',\n",
       " '3)',\n",
       " '3)C',\n",
       " '3)C.',\n",
       " '3)C.[',\n",
       " '3)C.[C',\n",
       " '3)C.[Cl',\n",
       " '3)C.[Cl-',\n",
       " '3)C.[Cl-]',\n",
       " ')C',\n",
       " ')C.',\n",
       " ')C.[',\n",
       " ')C.[C',\n",
       " ')C.[Cl',\n",
       " ')C.[Cl-',\n",
       " ')C.[Cl-]',\n",
       " 'C.',\n",
       " 'C.[',\n",
       " 'C.[C',\n",
       " 'C.[Cl',\n",
       " 'C.[Cl-',\n",
       " 'C.[Cl-]',\n",
       " '.[',\n",
       " '.[C',\n",
       " '.[Cl',\n",
       " '.[Cl-',\n",
       " '.[Cl-]',\n",
       " '[C',\n",
       " '[Cl',\n",
       " '[Cl-',\n",
       " '[Cl-]',\n",
       " 'Cl',\n",
       " 'Cl-',\n",
       " 'Cl-]',\n",
       " 'l-',\n",
       " 'l-]',\n",
       " '-]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substrings(queries, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayane/miniconda3/envs/rk/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-07-28 09:17:01 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-07-28 09:17:02 | INFO | numexpr.utils | NumExpr defaulting to 4 threads.\n",
      "2022-07-28 09:17:02 | INFO | fairseq.file_utils | loading archive file /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/input0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/good/gayane/data/chkpt/BBBP_bs_16_dropout_0.1_lr_3e-5_totalNum_1020_warmup_163_noise_type_uniform_r3f_lambda_1.0/checkpoint_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 09:17:10 | INFO | fairseq.tasks.denoising | dictionary: 1025 types\n",
      "2022-07-28 09:17:17 | INFO | fairseq.data.data_utils | loaded 204 examples from: /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/input0/valid\n",
      "2022-07-28 09:17:17 | INFO | fairseq.data.data_utils | loaded 204 examples from: /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/label/valid\n",
      "2022-07-28 09:17:17 | INFO | fairseq.data.data_utils | loaded 204 examples from: /home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/processed/label/valid\n",
      "/tmp/ipykernel_323375/3928296069.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  smi.append(bart.decode(torch.tensor(sm)))\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data.data_utils import load_indexed_dataset\n",
    "from fairseq.models.bart import BARTModel\n",
    "from fairseq.data import Dictionary\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "dataset_name = 'BBBP'\n",
    "# dataset = dataset_name if dataset_name in set([\"BBBP\", \"BACE\", \"HIV\"]) else f\"{dataset_name}_{args.subtask}\"\n",
    "dataset = dataset_name\n",
    "\n",
    "store_path = \"/home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data\"\n",
    "model = f\"{store_path}/{dataset}/processed/input0\"\n",
    "\n",
    "with open('/home/gayane/BartLM/captum/fairseq/scripts/datasets.json') as f:\n",
    "    datasets_json = json.load(f)\n",
    "dataset_js = datasets_json[dataset]\n",
    "task_type = dataset_js['type']\n",
    "\n",
    "if task_type == \"regression\":\n",
    "    mi = dataset_js['minimum']\n",
    "    ma = dataset_js['maximum']\n",
    "\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/\")\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/processed/\")\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/processed/input0/\")\n",
    "os.system(f\"mkdir -p {store_path}/{dataset}/processed/label/\")\n",
    "\n",
    "data_type = \"valid\"\n",
    "\n",
    "warmup = 163\n",
    "totNumUpdate = 1020\n",
    "lr = '3e-5'\n",
    "chkpt_path = '/mnt/good/gayane/data/chkpt/BBBP_bs_16_dropout_0.1_lr_3e-5_totalNum_1020_warmup_163_noise_type_uniform_r3f_lambda_1.0/checkpoint_best.pt'\n",
    "# chkpt_path = f\"/mnt/good/gayane/data/chkpt/{dataset}_bs_16_lr_{lr}_totalNum_{totNumUpdate}_warmup_{warmup}/checkpoint_last.pt\"\n",
    "print(chkpt_path)  # BACE_bs_16_lr_3e-5_totalNum_1135_warmup_181/ in test \n",
    "# bart = BARTModel.from_pretrained(model,  checkpoint_file = chkpt_path, \n",
    "bart = BARTModel.from_pretrained(model,  checkpoint_file = '/home/gayane/BartLM/checkpoints/checkpoint_last.pt', \n",
    "                                 bpe=\"sentencepiece\",\n",
    "                                 sentencepiece_model=\"/home/gayane/BartLM/Bart/chemical/tokenizer/chem.model\")\n",
    "bart.eval()\n",
    "bart.cuda(device=1)\n",
    "\n",
    "\n",
    "input_dict = Dictionary.load(f\"{store_path}/{dataset}/processed/input0/dict.txt\")\n",
    "smiles = list(load_indexed_dataset(\n",
    "    f\"{store_path}/{dataset}/processed/input0/{data_type}\", input_dict))\n",
    "\n",
    "\n",
    "test_label_path = f\"{store_path}/{dataset}/processed/label/{data_type}\"\n",
    "\n",
    "if task_type == 'classification':\n",
    "    \n",
    "    target_dict = Dictionary.load(f\"{store_path}/{dataset}/processed/label/dict.txt\")\n",
    "    targets = list(load_indexed_dataset(test_label_path, target_dict))\n",
    "elif task_type == 'regression':\n",
    "    with open(f'{test_label_path}.label') as f:\n",
    "        lines = f.readlines()\n",
    "        targets = [float(x.strip()) for x in lines]\n",
    "if task_type == 'classification':\n",
    "    if len(dataset_js[\"class_index\"])>1:\n",
    "        target_dict = list()\n",
    "        targets_list = list()\n",
    "        for i in range(len(dataset_js[\"class_index\"])):\n",
    "            target_dict.append(Dictionary.load(f\"{store_path}/{dataset}_{i}/processed/label/dict.txt\"))\n",
    "            targets_list.append(list(load_indexed_dataset(test_label_path[i], target_dict[i])))\n",
    "        \n",
    "    else: \n",
    "        target_dict = Dictionary.load(f\"{store_path}/{dataset}/processed/label/dict.txt\")\n",
    "        targets = list(load_indexed_dataset(test_label_path, target_dict))\n",
    "elif task_type == 'regression':\n",
    "    with open(f'{test_label_path}.label') as f:\n",
    "        lines = f.readlines()\n",
    "        targets = [float(x.strip()) for x in lines]\n",
    "\n",
    "import re\n",
    "smi = []\n",
    "for sm in smiles:\n",
    "    smi.append(bart.decode(torch.tensor(sm)))\n",
    "\n",
    "_input = pd.read_csv(f\"\"\"/home/gayane/BartLM/Bart/chemical/checkpoints/evaluation_data/BBBP/raw/{data_type}.input\"\"\", names=['SMILES'], header=None)\n",
    "li = _input[\"SMILES\"].to_list()\n",
    "# print(li[:2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = _input[\"SMILES\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[H+].C2=C1C(OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input[\"SMILES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = _input[\"SMILES\"].to_list()\n",
    "# for sml in range(len(li)):\n",
    "#     # li[sml] = li[sml][:122] + \"<mask>\"\n",
    "#     if \"(\" in li[sml]:\n",
    "#         li[sml] = li[sml][:122].replace('(', \"<mask>\", 1)\n",
    "\n",
    "\n",
    "# print(li)\n",
    "inputs = ['C(=O)(OC(C)(C)C)CCCc1ccc(<mask>1)N(CCCl)CCCl', 'C [ C <mask> ]( C c 1 ccc ( O ) c ( O ) c 1 ) ( N N ) C ( O )= O'] #li \n",
    "results = bart.fill_mask(masked_inputs = inputs,  topk=2, match_source_len=True)\n",
    "assert len(inputs) == len(results), \"different sizes?\"\n",
    "\n",
    "# d = {\"Original SMILES\": l, \"masked SMILES\": li, \"filled SMILES\": results}\n",
    "# df = pd.DataFrame(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  63,  11,  10, 357,   3,   4,  93,   4,   6,   4,   7,   8,   4,\n",
       "         12,  26,  79,   4,   4,  29,   4,  13,  39,   5,  26,   4,   5,   7,\n",
       "          4, 168,   4,   4,  29,   4,   4,  29,   4,  35,   4,   3,  11,  39,\n",
       "        156,   2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.encode('[H+].C2=C1C(OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[H+]<unk>CC1C(OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C<unk>[Cl-]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.decode(bart.encode(\"[H+].CC1C(OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cl'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.decode(torch.tensor([39]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[H+]<unk>CC1C(OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C<unk>[Cl-]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.decode(bart.encode(\"[H+].CC1C(OC(=NC1=CC=C2Cl)NCC)(C3=CC=CC=C3)C.[Cl-]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2='"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([ 0, 20, 12,  8,  5,  7,  8,  4,  7,  4,  5,  7,  4,  5,  4,  5,  4,  4,\n",
    "         4,  9,  6, 25,  7,  3,  6,  5, 14,  7,  4,  4, 39,  5,  4,  4, 39,  2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,   63, 1024,    2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bart.encode(\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('C[C@@H](CNC(=O)OC(C)(C)C)CCCc1ccc(F)cc1 O=C',\n",
       "   tensor(-0.1258, device='cuda:0')),\n",
       "  ('C[C@H](CNC(=O)OC(C)(C)C)CCCc1ccc(F)cc1 O=C',\n",
       "   tensor(-0.1264, device='cuda:0'))],\n",
       " [('C[C@H](N)[C@@H](O)c1ccccc1 c1ccc(C(F)(F)F)c(C(F)(F)C(F)',\n",
       "   tensor(-0.2962, device='cuda:0')),\n",
       "  ('C[C@H](N)[C@@H](O)c1ccccc1 c1ccc(C(F)(F)F)c(C(F)(F)F)c(',\n",
       "   tensor(-0.3008, device='cuda:0'))]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('CCOP(=O)C(C)(C)C Cc1cc(C)c(C)c(C(=O)O)c(C)c1C(=O)O C(=O)CCCCCCCCCCC CCCOCC',\n",
       "   tensor(-0.5145, device='cuda:0')),\n",
       "  ('CCOP(=O)C(C)(C)C Cc1cc(C)c(C)c(C(=O)O)c(C)c1C(=O)O C(=O)CCCCCCCCCCC C1',\n",
       "   tensor(-0.5182, device='cuda:0'))],\n",
       " [('C[C@H](N)[C@@H](O)c1ccccc1 c1ccc(C(F)(F)F)c(C(F)(F)C(F)',\n",
       "   tensor(-0.2961, device='cuda:0')),\n",
       "  ('C[C@H](N)[C@@H](O)c1ccccc1 c1ccc(C(F)(F)F)c(C(F)(F)C(F)(',\n",
       "   tensor(-inf, device='cuda:0'))]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(input_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbe366c07ac444f4192367dd82fc1131301390d518e7f0101681dc5adc8fe16f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('rk': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
